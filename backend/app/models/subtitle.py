from __future__ import annotations

from datetime import datetime
from typing import Any, Optional

from pydantic import BaseModel, Field


class SubtitleSegment(BaseModel):
    """A single subtitle line aligned with a portion of a video asset."""

    id: str = Field(..., description="Unique identifier for the subtitle segment")
    asset_id: str = Field(..., description="Video asset identifier")
    project_id: str = Field(..., description="Project identifier")
    index: int = Field(..., ge=0, description="Sequential index of the subtitle segment")
    start: float = Field(..., ge=0.0, description="Start timestamp in seconds")
    end: float = Field(..., ge=0.0, description="End timestamp in seconds")
    text: str = Field(..., min_length=1, description="Subtitle text content")
    confidence: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Confidence score provided by the speech recognition model",
    )
    language: Optional[str] = Field(default=None, description="Language code of the transcript segment")
    speaker: Optional[str] = Field(default=None, description="Optional speaker label if diarisation is available")
    request_id: str = Field(..., description="Identifier of the transcription request that produced this segment")
    metadata: dict[str, Any] = Field(default_factory=dict, description="Additional provider-specific metadata")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    @property
    def duration(self) -> float:
        return max(self.end - self.start, 0.0)


class SubtitleTranscript(BaseModel):
    """Collection of subtitle segments generated by an AI transcription run."""

    id: str = Field(..., description="Identifier for this transcription run")
    asset_id: str = Field(..., description="Video asset identifier")
    project_id: str = Field(..., description="Project identifier")
    language: str = Field(default="auto", description="Detected or requested language code")
    source_model: str = Field(default="openai-whisper", description="Model used to generate the transcript")
    prompt: Optional[str] = Field(default=None, description="Optional instruction prompt supplied to the model")
    parameters: dict[str, Any] = Field(default_factory=dict, description="Request configuration captured for caching")
    usage: dict[str, Any] = Field(default_factory=dict, description="Token and billing usage metadata")
    text: str = Field(default="", description="Full transcript text concatenated from segments")
    segments: list[SubtitleSegment] = Field(default_factory=list, description="Ordered subtitle segments")
    cached: bool = Field(default=False, description="Whether this transcript was returned from cache")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: dict[str, Any] = Field(default_factory=dict, description="Additional metadata for diagnostics")

    @property
    def segment_count(self) -> int:
        return len(self.segments)

    @property
    def duration(self) -> float:
        if not self.segments:
            return 0.0
        return max(segment.end for segment in self.segments)


__all__ = ["SubtitleSegment", "SubtitleTranscript"]
